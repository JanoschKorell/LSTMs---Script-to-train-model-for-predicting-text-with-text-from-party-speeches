{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlMh4zsCMDJt",
        "outputId": "dc5811b0-7e10-4faa-964b-9efff5a18a7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cci2fQmnFuXD",
        "outputId": "d686f8ae-9501-43eb-8a2f-29c76821398a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam,RMSprop\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.models import load_model\n",
        "import numpy as np \n",
        "import numpy as np \n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "import re\n",
        "from nltk import tokenize\n",
        "import keras.utils as ku\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "from tensorflow.keras import mixed_precision\n",
        "policy = mixed_precision.Policy('mixed_float16')\n",
        "mixed_precision.set_global_policy(policy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yui-vpYr7of6"
      },
      "outputs": [],
      "source": [
        "#import data\n",
        "text_df = pd.read_pickle('/content/gdrive/MyDrive/files/df_parteien.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k79Vkn_cehKu"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "#choose party\n",
        "partei = 'SPD'"
      ],
      "metadata": {
        "id": "QIK6GX5stnce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hokrg6s11Bbd"
      },
      "outputs": [],
      "source": [
        "#select only speeches from party\n",
        "text_df = text_df.sort_values(by='Sitzung')\n",
        "text_df = text_df.loc[text_df[partei] ==1 ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIhS_2R41BeR"
      },
      "outputs": [],
      "source": [
        "def clean_text(doc):\n",
        "    \n",
        "    doc = re.sub(r'\\-\\\\n','', doc, flags=re.DOTALL  | re.MULTILINE).strip() \n",
        "    doc = re.sub(r'\\-\\n','', doc, flags=re.DOTALL  | re.MULTILINE).strip() \n",
        "    doc = re.sub(r'\\n',' ', doc, flags=re.DOTALL  | re.MULTILINE).strip() \n",
        "    doc = re.sub(r'([a-z])\\-  ([a-z])', r'\\1\\2', doc, flags=re.DOTALL  | re.MULTILINE).strip()  \n",
        "    doc = re.sub(r'(Herr|Frau)\\n?\\s?Präsidenti?n?[\\!\\.\\s\\,]','', doc, flags=re.DOTALL  | re.MULTILINE).strip() \n",
        "    doc = re.sub(r'Sehr geehrter?[\\!\\.\\s\\,]','', doc, flags=re.DOTALL  | re.MULTILINE).strip() \n",
        "    doc = re.sub(r'[Meine ]+Damen\\s?\\n?und\\s?\\n?Herren[\\!\\.\\s\\,]','', doc, flags=re.DOTALL  | re.MULTILINE).strip() \n",
        "    doc = re.sub(r'Kollege \\w+','', doc, flags=re.DOTALL  | re.MULTILINE).strip() \n",
        "    doc = re.sub(r'Vielen Dank\\,','', doc, flags=re.DOTALL  | re.MULTILINE).strip() \n",
        "    doc = re.sub(r'[\\!\\:\\?\\;]','.', doc, flags=re.DOTALL  | re.MULTILINE).strip() \n",
        "    doc = re.sub(r'[\\-\\–\\(\\)\\[\\]]','', doc, flags=re.DOTALL  | re.MULTILINE).strip() \n",
        "    doc = re.sub(r'\\\\.','.', doc, flags=re.DOTALL  | re.MULTILINE).strip() \n",
        "    doc = re.sub(r'\\.\\s\\s','.', doc, flags=re.DOTALL  | re.MULTILINE).strip() \n",
        "    doc = re.sub(r'\\\\:','', doc, flags=re.DOTALL  | re.MULTILINE).strip() \n",
        "    doc = re.sub(r'[\\*\\…\\'\\\"\\“\\”\\„\\ʼ\\’]',' ', doc, flags=re.DOTALL  | re.MULTILINE).strip() \n",
        "    doc = re.sub(r'   ',' ', doc, flags=re.DOTALL  | re.MULTILINE).strip() \n",
        "    doc = re.sub(r'\\d+',' ', doc, flags=re.DOTALL  | re.MULTILINE).strip() \n",
        "    doc = re.sub(r'  ',' ', doc, flags=re.DOTALL  | re.MULTILINE).strip() \n",
        "    doc = re.sub(r'\\s\\.','.', doc, flags=re.DOTALL  | re.MULTILINE).strip() \n",
        "    doc = re.sub(r'\\s\\,',',', doc, flags=re.DOTALL  | re.MULTILINE).strip() \n",
        "    doc = re.sub(r'(\\.)([A-z])', r'\\1 \\2', doc, flags=re.DOTALL  | re.MULTILINE).strip() \n",
        "\n",
        "    \n",
        "    return doc\n",
        "    \n",
        "text_df.loc[:,'clean_text'] = text_df.loc[:,'Reden_clean'].apply(clean_text)\n",
        "text_df.loc[:,'clean_text']\n",
        "\n",
        "df_small, features_test = train_test_split(text_df,test_size=0.9, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJxGvtHU1BhG"
      },
      "outputs": [],
      "source": [
        "#devide corpus in sentences\n",
        "corpus_1 = []\n",
        "corpus = []\n",
        "for text in df_small.loc[:,'clean_text']:\n",
        "    corpus_1.append(text)\n",
        "    \n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "for text in corpus_1:\n",
        "    for sen in sent_tokenize(text):\n",
        "        if len(sen) < 500:\n",
        "          if len(sen) > 4:\n",
        "            corpus.append(sen.lower())    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNUFX3K31BjY"
      },
      "outputs": [],
      "source": [
        "#tokenize words\n",
        "\n",
        "tokenizer = Tokenizer(filters='!\"#$%&()*+-/:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
        "\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "print(tokenizer.word_index)\n",
        "print(total_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-RE5tcQm1Bmu"
      },
      "outputs": [],
      "source": [
        "#text to sequences\n",
        "input_sequences = []\n",
        "for line in corpus:\n",
        "\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\tfor i in range(1, len(token_list)):\n",
        "\t\tn_gram_sequence = token_list[:i+1]\n",
        "\t\tinput_sequences.append(n_gram_sequence)\n",
        "\n",
        "# pad sequences \n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "# create predictors and label\n",
        "x, labels = input_sequences[:,:-1],input_sequences[:,-1]\n",
        "\n",
        "y = tf.keras.utils.to_categorical(labels, num_classes=total_words)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2tW_QIBk1BtZ"
      },
      "outputs": [],
      "source": [
        "#train model\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 750, input_length=max_sequence_len-1))\n",
        "model.add(Dropout(rate=0.3))\n",
        "model.add(Bidirectional(LSTM(375, return_sequences = True)))\n",
        "model.add(Dropout(rate=0.3))\n",
        "model.add(Bidirectional(LSTM(225, return_sequences = True)))\n",
        "model.add(LSTM(225, return_sequences = True))\n",
        "model.add(LSTM(225))\n",
        "model.add(Dropout(rate=0.3))\n",
        "model.add(Dense(375, activation='relu'))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "adam = Adam(learning_rate=0.001)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['Precision', 'Recall'])\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "early = EarlyStopping(monitor='loss', patience=50)\n",
        "checkpoint = ModelCheckpoint(monitor='loss',filepath='/content/gdrive/MyDrive/files/'+partei+'.h5')\n",
        "history = model.fit(x, y, epochs=650, batch_size=2048, callbacks=[checkpoint],verbose = 1) \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#open model\n",
        "from keras.models import load_model\n",
        "model = load_model('/content/gdrive/MyDrive/files/'+partei+'.h5')"
      ],
      "metadata": {
        "id": "2LhEoSxls_OI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73n79E8IFSUO"
      },
      "outputs": [],
      "source": [
        "#save model\n",
        "with open('/content/gdrive/MyDrive/files/'+partei+'1.pkl', 'wb') as f: \n",
        "        pickle.dump(model, f)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#save tokenizer and max_sequence_len_\n",
        "with open('/content/gdrive/MyDrive/files/tokenizer_'+partei, 'wb') as f: \n",
        "        pickle.dump(tokenizer, f)\n",
        "\n",
        "with open('/content/gdrive/MyDrive/files/max_sequence_len_'+partei, 'wb') as f: \n",
        "        pickle.dump(max_sequence_len, f)"
      ],
      "metadata": {
        "id": "VstRbU45s5VC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aarLmbqF6q-j"
      },
      "outputs": [],
      "source": [
        "#test result\n",
        "seed_text = \"\"\n",
        "next_words = 60\n",
        "  \n",
        "for _ in range(next_words):\n",
        "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\tpredicted = np.argmax(model.predict(token_list), axis=-1)\n",
        "\toutput_word = \"\"\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == predicted:\n",
        "\t\t\toutput_word = word\n",
        "\t\t\tbreak\n",
        "\tseed_text += \" \" + output_word\n",
        "display(seed_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POxfjAJs6qjz"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}